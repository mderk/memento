---
file: testing.md
target_path: .memory_bank/guides/
priority: 15
dependencies: []
conditional: null
---

# Generation Instructions for guides/testing.md

## Context

You are generating the testing guide - comprehensive documentation for testing philosophy, strategies, and best practices across the entire application.

This file serves as:
- **Testing philosophy** documentation
- **Test patterns** reference
- **Testing tools** guide
- **Coverage goals** and metrics

**Audience**: All developers
**Tone**: Technical, practical, quality-focused
**Length**: 400-600 lines (comprehensive)

## Input Data

```json
{
  "project_name": "string",

  "backend_framework": "string|null",
  "backend_test_framework": "pytest|jest|mocha|rspec|...",

  "frontend_framework": "string|null",
  "frontend_test_framework": "jest|vitest|testing-library|...",

  "has_backend": true|false,
  "has_frontend": true|false,

  "test_command": "string|null"
}
```

## Output Requirements

### Structure

Generate testing.md with these sections:

#### 1. Header
```markdown
# Testing Guide - {Project Name}

This guide covers testing philosophy, strategies, and best practices for {Project Name}.
```

#### 2. Testing Philosophy

**Why We Test:**
- Catch bugs before production
- Enable confident refactoring
- Document expected behavior
- Improve code design

**Testing Principles:**
1. **Test behavior, not implementation**
2. **Write tests first (TDD)**  or alongside code
3. **Keep tests simple and readable**
4. **Test one thing at a time**
5. **Fast feedback loop**

**Coverage Goals:**
- **Minimum**: 80% code coverage
- **Target**: 90% code coverage
- **Critical paths**: 100% coverage

#### 3. Test Pyramid

```
        ┌─────────┐
        │   E2E   │  ← Few: Slow, brittle, expensive
        └─────────┘
      ┌─────────────┐
      │ Integration │  ← Some: Medium speed and cost
      └─────────────┘
    ┌─────────────────┐
    │   Unit Tests    │  ← Many: Fast, cheap, reliable
    └─────────────────┘
```

**Distribution:**
- **70%** Unit tests (isolated, fast)
- **20%** Integration tests (components working together)
- **10%** E2E tests (full user flows)

#### 4. Unit Testing

**What to Unit Test:**
- Functions and methods
- Component logic
- Utility functions
- Business logic

**If has_backend:**

##### Backend Unit Tests

**Testing Framework**: {backend_test_framework}

**Example (pytest):**
```python
# tests/test_user_service.py
import pytest
from app.services import UserService
from app.models import User

class TestUserService:
    def test_create_user(self):
        service = UserService()
        user_data = {
            'email': 'test@example.com',
            'name': 'Test User'
        }

        user = service.create_user(user_data)

        assert user.email == 'test@example.com'
        assert user.name == 'Test User'

    def test_create_user_duplicate_email(self):
        service = UserService()
        user_data = {'email': 'test@example.com'}

        service.create_user(user_data)

        with pytest.raises(ValueError, match="Email already exists"):
            service.create_user(user_data)

    @pytest.mark.asyncio
    async def test_async_operation(self):
        result = await async_function()
        assert result == expected_value
```

**Fixtures:**
```python
# conftest.py
import pytest
from app.database import get_db

@pytest.fixture
def db():
    db = get_db()
    yield db
    db.rollback()
    db.close()

@pytest.fixture
def sample_user(db):
    user = User(email="test@example.com")
    db.add(user)
    db.commit()
    return user
```

**If has_frontend:**

##### Frontend Unit Tests

**Testing Framework**: {frontend_test_framework}

**Example (React Testing Library):**
```tsx
// Button.test.tsx
import { render, screen, fireEvent } from '@testing-library/react';
import { Button } from './Button';

describe('Button', () => {
  it('renders with text', () => {
    render(<Button>Click me</Button>);
    expect(screen.getByText('Click me')).toBeInTheDocument();
  });

  it('calls onClick when clicked', () => {
    const handleClick = jest.fn();
    render(<Button onClick={handleClick}>Click</Button>);

    fireEvent.click(screen.getByText('Click'));
    expect(handleClick).toHaveBeenCalledTimes(1);
  });

  it('is disabled when disabled prop is true', () => {
    render(<Button disabled>Click</Button>);
    expect(screen.getByText('Click')).toBeDisabled();
  });

  it('applies correct CSS class for variant', () => {
    render(<Button variant="primary">Click</Button>);
    expect(screen.getByText('Click')).toHaveClass('btn-primary');
  });
});
```

**Testing Hooks:**
```tsx
import { renderHook, act } from '@testing-library/react';
import { useCounter } from './useCounter';

describe('useCounter', () => {
  it('increments counter', () => {
    const { result } = renderHook(() => useCounter());

    act(() => {
      result.current.increment();
    });

    expect(result.current.count).toBe(1);
  });
});
```

#### 5. Integration Testing

**What to Integration Test:**
- API endpoints
- Database operations
- Component interactions
- Service integrations

**If has_backend:**

##### Backend Integration Tests

```python
# tests/integration/test_user_api.py
import pytest
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)

def test_create_user_endpoint():
    response = client.post('/api/users/', json={
        'email': 'test@example.com',
        'name': 'Test User'
    })

    assert response.status_code == 201
    data = response.json()
    assert data['email'] == 'test@example.com'

def test_get_user_endpoint():
    # Create user first
    create_response = client.post('/api/users/', json={
        'email': 'test@example.com'
    })
    user_id = create_response.json()['id']

    # Get user
    response = client.get(f'/api/users/{user_id}')

    assert response.status_code == 200
    assert response.json()['id'] == user_id

def test_authentication_required():
    response = client.get('/api/protected-resource/')
    assert response.status_code == 401
```

**If has_frontend:**

##### Frontend Integration Tests

```tsx
// UserList.integration.test.tsx
import { render, screen, waitFor } from '@testing-library/react';
import userEvent from '@testing-library/user-event';
import { QueryClient, QueryClientProvider } from '@tanstack/react-query';
import { UserList } from './UserList';
import { server } from '../mocks/server';

const queryClient = new QueryClient({
  defaultOptions: { queries: { retry: false } }
});

describe('UserList Integration', () => {
  beforeAll(() => server.listen());
  afterEach(() => server.resetHandlers());
  afterAll(() => server.close());

  it('loads and displays users', async () => {
    render(
      <QueryClientProvider client={queryClient}>
        <UserList />
      </QueryClientProvider>
    );

    expect(screen.getByText('Loading...')).toBeInTheDocument();

    await waitFor(() => {
      expect(screen.getByText('John Doe')).toBeInTheDocument();
      expect(screen.getByText('Jane Smith')).toBeInTheDocument();
    });
  });

  it('creates new user', async () => {
    const user = userEvent.setup();

    render(
      <QueryClientProvider client={queryClient}>
        <UserList />
      </QueryClientProvider>
    );

    await user.click(screen.getByText('Add User'));
    await user.type(screen.getByLabelText('Name'), 'New User');
    await user.click(screen.getByText('Save'));

    await waitFor(() => {
      expect(screen.getByText('New User')).toBeInTheDocument();
    });
  });
});
```

#### 6. End-to-End Testing

**What to E2E Test:**
- Critical user flows
- Multi-page workflows
- Payment processes
- Authentication flows

**Tools**: Playwright, Cypress, Selenium

**Example (Playwright):**
```typescript
// e2e/login.spec.ts
import { test, expect } from '@playwright/test';

test('user can log in', async ({ page }) => {
  await page.goto('http://localhost:3000');

  // Navigate to login
  await page.click('text=Login');

  // Fill form
  await page.fill('input[name="email"]', 'test@example.com');
  await page.fill('input[name="password"]', 'password123');

  // Submit
  await page.click('button[type="submit"]');

  // Verify redirect to dashboard
  await expect(page).toHaveURL(/.*dashboard/);
  await expect(page.locator('h1')).toHaveText('Dashboard');
});

test('shows error for invalid credentials', async ({ page }) => {
  await page.goto('http://localhost:3000/login');

  await page.fill('input[name="email"]', 'wrong@example.com');
  await page.fill('input[name="password"]', 'wrongpassword');
  await page.click('button[type="submit"]');

  await expect(page.locator('.error')).toHaveText('Invalid credentials');
});
```

#### 7. Test Data Management

**Factories/Builders:**
```python
# tests/factories.py
import factory
from app.models import User

class UserFactory(factory.Factory):
    class Meta:
        model = User

    email = factory.Sequence(lambda n: f'user{n}@example.com')
    name = factory.Faker('name')
    age = factory.Faker('pyint', min_value=18, max_value=80)

# Usage
user = UserFactory.create()
users = UserFactory.create_batch(10)
```

**Mocking:**
```python
# Mock external service
from unittest.mock import Mock, patch

@patch('app.services.email_service.send_email')
def test_user_creation_sends_email(mock_send_email):
    service = UserService()
    service.create_user({'email': 'test@example.com'})

    mock_send_email.assert_called_once()
    args = mock_send_email.call_args[0]
    assert 'test@example.com' in args
```

**Frontend Mocking (MSW):**
```ts
// mocks/handlers.ts
import { rest } from 'msw';

export const handlers = [
  rest.get('/api/users', (req, res, ctx) => {
    return res(
      ctx.status(200),
      ctx.json([
        { id: 1, name: 'John Doe' },
        { id: 2, name: 'Jane Smith' }
      ])
    );
  }),

  rest.post('/api/users', (req, res, ctx) => {
    return res(
      ctx.status(201),
      ctx.json({ id: 3, ...req.body })
    );
  })
];
```

#### 8. Running Tests

**Commands:**
```bash
# Run all tests
{test_command}

# Run specific test file
pytest tests/test_user_service.py
npm test -- UserList.test.tsx

# Run with coverage
pytest --cov=app tests/
npm test -- --coverage

# Run in watch mode
pytest-watch
npm test -- --watch

# Run E2E tests
playwright test
cypress run
```

**CI/CD Integration:**
```yaml
# .github/workflows/test.yml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run tests
        run: |
          {test_command}
          # Check coverage threshold
          pytest --cov=app --cov-fail-under=80 tests/
```

#### 9. Test Coverage

**Measuring Coverage:**
```bash
# Backend
pytest --cov=app --cov-report=html tests/
open htmlcov/index.html

# Frontend
npm test -- --coverage
open coverage/lcov-report/index.html
```

**Coverage Goals:**
- **Critical Code**: 100% (payment, auth, security)
- **Business Logic**: 95%
- **UI Components**: 80%
- **Utilities**: 90%

**What Not to Test:**
- Third-party libraries
- Framework code
- Generated code
- Trivial getters/setters

#### 10. Test Organization

**Directory Structure:**
```
tests/
├── unit/
│   ├── services/
│   ├── models/
│   └── utils/
├── integration/
│   ├── api/
│   └── database/
├── e2e/
│   ├── auth.spec.ts
│   └── checkout.spec.ts
├── fixtures/
│   └── users.py
├── mocks/
│   └── handlers.ts
└── conftest.py  # pytest configuration
```

**Naming Conventions:**
- Test files: `test_*.py` or `*.test.tsx`
- Test functions: `test_what_it_does`
- Test classes: `TestClassName`

#### 11. Best Practices

**DO:**
- ✅ Test behavior, not implementation
- ✅ Use descriptive test names
- ✅ Keep tests independent
- ✅ Mock external dependencies
- ✅ Write tests alongside code
- ✅ Aim for high coverage
- ✅ Run tests before committing

**DON'T:**
- ❌ Test private methods directly
- ❌ Make tests depend on each other
- ❌ Use real external services in tests
- ❌ Write tests after the fact
- ❌ Ignore failing tests
- ❌ Test third-party code
- ❌ Skip edge cases

#### 12. Debugging Tests

**Common Issues:**

**Flaky Tests:**
- Timing issues → Use waitFor, await
- Random data → Use fixed seeds
- External dependencies → Mock them

**Slow Tests:**
- Database operations → Use in-memory DB
- API calls → Mock responses
- Too many E2E tests → Move to integration

**Failing Tests:**
- Read error message carefully
- Check test isolation
- Verify fixtures and mocks
- Run single test in isolation

#### 13. Test-Driven Development (TDD)

**Red-Green-Refactor:**
1. **Red**: Write failing test
2. **Green**: Write minimal code to pass
3. **Refactor**: Improve code quality

**Example:**
```python
# 1. RED: Write test first
def test_calculate_discount():
    result = calculate_discount(price=100, discount_percent=20)
    assert result == 80

# 2. GREEN: Minimal implementation
def calculate_discount(price, discount_percent):
    return price - (price * discount_percent / 100)

# 3. REFACTOR: Improve
def calculate_discount(price: float, discount_percent: float) -> float:
    """Calculate price after discount."""
    if not 0 <= discount_percent <= 100:
        raise ValueError("Discount must be between 0 and 100")
    return price * (1 - discount_percent / 100)
```

#### 14. Performance Testing

**Load Testing:**
- Use tools like Locust, Artillery, k6
- Test API under load
- Identify bottlenecks

**Benchmarking:**
```python
import pytest

def test_function_performance(benchmark):
    result = benchmark(expensive_function, arg1, arg2)
    assert result == expected_value
```

#### 15. Accessibility Testing

**Automated A11y Tests:**
```tsx
import { render } from '@testing-library/react';
import { axe, toHaveNoViolations } from 'jest-axe';

expect.extend(toHaveNoViolations);

test('Button has no accessibility violations', async () => {
  const { container } = render(<Button>Click me</Button>);
  const results = await axe(container);

  expect(results).toHaveNoViolations();
});
```

#### 16. References

**Testing Libraries:**
- {backend_test_framework} documentation
- {frontend_test_framework} documentation
- Testing best practices

**Internal Documentation:**
- [Backend Guide](./backend.md) (if has_backend)
- [Frontend Guide](./frontend.md) (if has_frontend)
- [Code Review Guidelines](./code-review-guidelines.md)

### Conditional Logic

**If has_backend:**
Include backend testing section with backend_test_framework examples

**If has_frontend:**
Include frontend testing section with frontend_test_framework examples

**If test_command exists:**
Use it in "Running Tests" section

### Style Guidelines

1. **Practical examples**: Real, runnable test code
2. **Framework-specific**: Use actual test frameworks
3. **Comprehensive**: Cover all test types
4. **Best practices**: Explain why, not just how
5. **Visual aids**: Use ASCII diagrams for concepts
6. **Clear organization**: Logical flow from unit → integration → E2E
7. **No placeholder disclaimers** (CRITICAL):
   - NEVER generate: "Current Status: not configured yet", "when set up", "future implementation"
   - ALWAYS assume frameworks are installed
   - Provide clear installation commands at the top
   - Show run commands as if everything is ready
   - Let command failures speak for themselves (agent will understand)

### Quality Checklist

- [ ] Testing philosophy explained clearly
- [ ] Test pyramid diagram included
- [ ] Unit test examples for detected frameworks
- [ ] Integration test examples
- [ ] E2E test examples
- [ ] Test data management (factories, mocks)
- [ ] Running tests commands
- [ ] Coverage goals specified
- [ ] Best practices DO/DON'T list
- [ ] TDD workflow explained
- [ ] References link to valid guides
- [ ] No {{PLACEHOLDERS}} remain
- [ ] **No placeholder status disclaimers** ("not configured yet", "when set up", "future implementation")
- [ ] **Assumes frameworks installed**, provides clear install commands
- [ ] 400-600 lines comprehensive
- [ ] Code examples are syntactically correct

## Common Mistakes to Avoid

1. ❌ Wrong test framework examples
2. ❌ Not adapting to detected frameworks
3. ❌ Missing test pyramid explanation
4. ❌ No coverage goals
5. ❌ Backend examples when has_backend is false
6. ❌ Frontend examples when has_frontend is false
7. ❌ Generic examples that don't match project
8. ❌ Too short (< 300 lines)
9. ❌ Missing TDD section
10. ❌ No debugging guidance
11. ❌ **Placeholder status disclaimers** ("not configured yet", "when set up", "future implementation")

## Validation

- [ ] Test frameworks match detected tools
- [ ] Examples use correct syntax for frameworks
- [ ] Conditional sections handled correctly
- [ ] Coverage goals are specific numbers
- [ ] All test types explained (unit, integration, E2E)
- [ ] Running tests section has actual commands
- [ ] Best practices clearly explained
- [ ] References are valid
- [ ] 400-600 lines
- [ ] No placeholders
- [ ] Grammar perfect
